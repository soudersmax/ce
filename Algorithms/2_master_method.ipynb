{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pending-compound",
   "metadata": {},
   "source": [
    "# The Master Method\n",
    "* [Motivation](#mot)\n",
    "* [Formal Statement](#form)\n",
    "* [Examples](#ex)\n",
    "* [Proof Part 1](#p1)\n",
    "* [Interpretation of the Three Cases](#int)\n",
    "* [Proof Part 2](#p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-latin",
   "metadata": {},
   "source": [
    "## Motivation <a class=\"anchor\" id=\"mot\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-sensitivity",
   "metadata": {},
   "source": [
    "* Potentially useful algorithmic ideas often need mathematical analysis to evaluate\n",
    "    * ie: the grade-school multiplication algrorithm uses O(n<sup>2</sup>) operations to multiply two n-digit numbers vs.  the recursive approach using divide and conquer to break those n-digit numbers into smaller n/2 digit numbers\n",
    "* The way we reason about the running time of recursive algorithms uses a concept called __recurrence__\n",
    "* __Recurrence__ - a way to express T(n) in terms of T smaller numbers, express T(n) in terms of running time of recursive calls\n",
    "    * 2 ingredients:\n",
    "        1. Base Case - Once you get down to a small input, then the running time is just constant\n",
    "        2. General Case - Running time is comprised of the work done by the recursive calls and the work done right now, outside the recursive calls\n",
    "<br><br>\n",
    "* Integer Multiplication Example \n",
    "    > (Statement * ): x * y = 10<sup>n</sup>ac + 10<sup>n/2></sup>(ad + bc) + bd\n",
    "\n",
    "* ___Previously, we focused on the work being done OUTSIDE recursive calls for out examples and proofs (merge sort, inversion counts) but this analysis focuses on the work done INSIDE the recursive calls___\n",
    "* Grade School (Naive) Version:\n",
    "    * Recursively compute ac, ad, bc, bd and complete Statement * in the natural way, adding zeros as necessary and summing the individual parts.\n",
    "    * T(n) = maximum number of operations this algorithm needs to multiply two n-digit numbers\n",
    "    * Base Case - In this case, 2 1-digit numbers = T(1) <= a constant\n",
    "    * General Case - 4 recursive calls  = 4T(n/2)), additions etc outside the recursive calls = (O(n)) --> T(n) <= 4T(n/2) + O(n)\n",
    "* Gauss Version (Optimized Karatsuba):\n",
    "    * Recursively compute ac, bd, (a+b)(c+d), and the final product is (a+b)(c+d) - ac - bd\n",
    "    * Base Case - 2 1-digit numbers, --> T(1) is a constant\n",
    "    * Recursive case - __3__ recursive calls, plus additions outside the recursion --> 3T(n/2) + O(n)\n",
    "* In both of these examples, we are ignoring two things:\n",
    "    1. The number of digits in the sums of (a + b) and (c + d) may result in numbers with n/2 +1 digits, BUT that additional 1 makes little to no difference in the analysis\n",
    "    2. The exact constant factor in the linear work done outside the recursive calls. This is actually a little larger in Gauss's version, but because it's a constant factor, would be suppressed in the Big Oh notation anyway\n",
    "* Consider the __Merge Sort__ - instead of 3 recursive calls, it makes 2 (one on the left and one on the right), and outside of those calls it also does linear work. \n",
    "    * We know that the running time is N log n, so Gauss's algorithm is going to be worse --> we just don't know by how much\n",
    "    * Thus, we don't actually have any idea what the running time of Gauss's Algorithm is, we just know some things that it sits between\n",
    "<br>\n",
    "<br>\n",
    "* ___Motivation: The running time/recurrence of Gauss's recursive algorithm for integer multplication can be explicitly solved via the Master Method___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-romantic",
   "metadata": {},
   "source": [
    "## Formal Statement <a class=\"anchor\" id=\"form\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-world",
   "metadata": {},
   "source": [
    "* The __Master Method__ is essentially a black box for solving recurrences\n",
    "    * Input: a recurrence in a particular format\n",
    "    * Output: Solution, an upper bound of the running time of the recursive algorithm \n",
    "* Assumptions for our purposes:\n",
    "    1. All subproblems have equal size\n",
    "* Necessary Recurrence Format\n",
    "    1. Base Case: T(n) <= a constant for all sufficiently small n\n",
    "    2. General case: The running time of an input of length n is bounded above by some constant (a) of recursive calls and then each of those sub problems has exactly the same size of (1/b) fraction of the original input size. Outside the recursive calls, there is some amount of work, so we'll call that n<sup>d</sup>\n",
    "        * T(n) <= aT(n/b) + O(n<sup>d</sup>\n",
    "        * where constants:\n",
    "            * a = number of recursive calls(>= 1)\n",
    "            * b is the factor by which the input size shrinks before recursive calls are called (>1)\n",
    "            * d = exponent of the \"combine\" step (>= 0)\n",
    "    * There _is_ a constant hidden in the big-oh notation of d, but it doesn't really matter in the full analysis. Will be accounted for in the actual proof.\n",
    "<br><br>\n",
    "___The Master Method Formal Statement___  \n",
    "* 3 cases:\n",
    "    * a = b<sup>d</sup>\n",
    "        * T(n) = O(n<sup>d</sup>logn))\n",
    "    * a < b<sup>d</sup>\n",
    "        * T(n) = O(n<sup>d</sup>)\n",
    "    * a > b<sup>d</sup>\n",
    "        * T(n) = O(n<sup>log<sub>b</sub>a</sup>)\n",
    "* Observations\n",
    "    * In case 1, we don't specify the log _because it doesn't matter_. The log with respect to any two bases differs by a constant factor independent of n, which is (of course) suppressed in notation. This is opposed to case 3, where we have a log in the exponent, wherein a constant is _much_ more important in keeping track of the exponential time of the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-chile",
   "metadata": {},
   "source": [
    "## Examples <a class=\"anchor\" id=\"ex\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-wayne",
   "metadata": {},
   "source": [
    "#### Example 1 - Merge Sort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-october",
   "metadata": {},
   "source": [
    "* Identify the values a, b, and d\n",
    "    * a = 2 (number of recursive calls)\n",
    "    * b = 2 (each sub problem is half the size of the original input)\n",
    "    * d = 1 (work done outside of recursive calls is in linear time)\n",
    "* Identify the case\n",
    "    * a = 2\n",
    "    * b<sup>d</sup> = 2<sup>1</sup> = 2\n",
    "    * a = b --> Case 1\n",
    "* In case 1, T(n) = T(n) = O(n<sup>1</sup>logn)) = O(n log n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-sacrifice",
   "metadata": {},
   "source": [
    "#### Example 2 - Binary Search Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-council",
   "metadata": {},
   "source": [
    "* In binary search, you don't have to check both sides of the array. Just look at the middle element, compare it to the target and then recurse on the appropriate side of the array\n",
    "    * a = 1\n",
    "* Each subproblem is again divided by two\n",
    "    * b = 2\n",
    "* Outside of recursive call is 1 comparison, so it's constant \n",
    "    * d = 0\n",
    "* Case 1 --> T(n) = O(n<sup>d</sup>logn)) = O(log n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-intro",
   "metadata": {},
   "source": [
    "#### Example 3 - Naive Int. Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-moderator",
   "metadata": {},
   "source": [
    "* 4 Recursive calls\n",
    "    * a = 4\n",
    "* When we take products, each smaller number has n/2 digits\n",
    "    * b = 2\n",
    "* Outside of the recursive calls we have a number of addition operations, which can be completed in linear time\n",
    "    * d = 1\n",
    "* 4 > 2<sup>1</sup> --> Case 3\n",
    "* Case 3 --> T(n) = O(n<sup>log<sub>b</sub>a</sup>)\n",
    "    * T(n) = O(n<sup>log<sub>2</sub>4</sup>) --> log<sub>2</sub>4 = 2 --> O(n<sup>2</sup>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-prize",
   "metadata": {},
   "source": [
    "#### Example 4 - Gauss's Int. Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-windsor",
   "metadata": {},
   "source": [
    "* a = 3 (recursive calls)\n",
    "* b = 2 (each number has n/2 digits)\n",
    "* d = 1 (linear work outside of calls for addition etc)\n",
    "* 3 > 2<sup>1</sup> --> Case 3\n",
    "* Case 3 --> T(n) = O(n<sup>log<sub>b</sub>a</sup>)\n",
    "    * T(n) = O(n<sup>log<sub>2</sub>3</sup>) = O(n<sup>1.59</sup>)\n",
    "* Despite still being Case 3, it's better than the quadratic time of Naive Integer Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-eleven",
   "metadata": {},
   "source": [
    "#### Example 5 - Strassen's Subcubic Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-currency",
   "metadata": {},
   "source": [
    "* Remember that In general, matrix multiplication requires 8 recursive calls, but if you're clever you can reduce that to 7 recursive calls\n",
    "* a = 7\n",
    "* b = 2 (half original)\n",
    "* d = 2 (linear time in the dimension of the matrix, which means that since the matrix is n * n size, it's quadratic)\n",
    "* Case 3 - 7 > 4\n",
    "    * T(n) = O(n<sup>log<sub>b</sub>a</sup>) --> O(n<sup>log<sub>2</sub>7</sup>) --> O(n<sup>2.81</sup>), which again beats the original which was cubic time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-outside",
   "metadata": {},
   "source": [
    "#### Example 6 - Fictitious recurrence for Case 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-quantity",
   "metadata": {},
   "source": [
    "* Case 2 happens, we just haven't covered any examples of it yet\n",
    "* Fictitious recurrence:\n",
    "    * T(n) <= 2T(n/2) + O(n<sup>2</sup>)\n",
    "* a = 2\n",
    "* b = 2\n",
    "* d = 2\n",
    "* Case 2: a < b<sup>d</sup> --> 2 < 4\n",
    "    * T(n) = O(n<sup>d</sup>) --> T(n) = O(n<sup>2</sup>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-safety",
   "metadata": {},
   "source": [
    "## Proof Part 1 <a class=\"anchor\" id=\"p1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-exhibit",
   "metadata": {},
   "source": [
    "* The computations are not the important part here, but they are worth seeing once. What _is_ important is ___the conceptual meaning of the three cases of the master method__.\n",
    "    * The proof will follo a recursion tree approach, just like we used in the merge sort analysis, with each case applying a certain type of recursion tree\n",
    "    * Remembering the recursion trees/cases witll allow you to 'reverse engineer' the running times from the conceptual understanding of what the three case mean and how they correspond to different trees\n",
    "* __Assumptions:__\n",
    "    1. Recurrence is in the following form:\n",
    "        * (i) T(1) <= c\n",
    "        * (ii) T(n) <= aT(n/b) + cn<sup>d</sup>\n",
    "        * Where c is some constant suppressed in big-oh notation, a = the number of recursive calls, b is the factor by which the subproblems are divided and d is the work outside the recursive calls\n",
    "    2. n = a power of b (Just to make our lives easier, it doens't really affect the proof)\n",
    "* __General Idea:__\n",
    "    * Generalize merge sort analysis (ie: recursion tree)  \n",
    "<img src=\"resources/rec_tree.PNG\">\n",
    "* __Quiz Question__: At a given level j (= 0, 1, 2,....,log<sub>b</sub>n), how many distinct subproblems are there? What is the input size of each of those subproblems?\n",
    "    * a<sup>j</sup> subproblems - Starting at level 0 (j = 0), each further problem makes _a_ further calls, so the number of problems increases by a factor of a\n",
    "    * Subproblems have size n/b<sup>j</sup> - the input into each subproblem is decreased by a factor of j\n",
    "    * These are not always the same - there are cases where a tree might recursively call 2 subproblems, but the input is not necessarily the exact output from the initial call\n",
    "        * Ex: A function makes two recursive calls, each covering half the list. The list isn't actually shrunk but continues to undergo a given function (like a transformation or something), so even though the number of subproblems may increase by two each time, each subproblem is still taking n/2 (the initial half from the first call)\n",
    "* ___Generalized Recursion Tree___\n",
    "<img src=\"resources/master_tree.PNG\">\n",
    "1. Zoom into a single level j and calculate the total work done _not including_ that which would be done later by recursive calls\n",
    "    * Look at the number of problems at level j and multiply that by a bound on the work done per sub-problem\n",
    "        1. a<sup>j</sup> problems per level as we saw above\n",
    "        2. Subproblem size = n/b<sup>j</sup>.\n",
    "            * Size matters only inasmuch as it determines the amount of work (number of operations performed) per level j subproblem. This relationship is found in the recurrence\n",
    "            * _Recurrence_ = how much work is done in a given subproblem. Includes both the recursive call work and that done outside of calls. \n",
    "            * We said above to ignore the parts done in the recursive calls and just count the work done at level j --> <= c * input size raised to the d power \n",
    "            * (n/b<sup>j</sup>)<sup>d</sup> * c\n",
    "    * Now that we have the two parts, combine: \n",
    "        * <= a<sup>j</sup> * c * (n/b<sup>j</sup>)<sup>d</sup>\n",
    "    * Simplify into those terms which are dependent on level j and those independent of j\n",
    "    * a and b are both expressions of j, while c and n<sup>d</sup> are independent\n",
    "        * cn<sup>d</sup> * (a/b<sup>d</sup>)<sup>j</sup>\n",
    "    * We now have the relative magnitude of a, b, and d, the key players in master theorem\n",
    "2. Sum over all the levels j=0, 1, 2,..., log<sub>b</sub>n\n",
    "    * Because cn<sup>d</sup> is not dependent on j, we can pull it out ahead of the summation expression\n",
    "<img src=\"resources/proof1_sum.PNG\">\n",
    "<blockquote>This somewhat messy formula is <i>crucial</i> to the rest of the proof and the 3 cases</blockquote>  \n",
    "\n",
    "#### What did we just do?\n",
    "* We used a recursion tree approach which gave us an upper bound of a running time of an algorithm, governed by the recurrence in the specified form.\n",
    "* This very ugly formula now needs to be interpreted into the 3 cases of the Master Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-question",
   "metadata": {},
   "source": [
    "## Interpretation of the Three Cases <a class=\"anchor\" id=\"int\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-accident",
   "metadata": {},
   "source": [
    "* The entirety of the Master Theorem revolves around the relationship between a and b<sup>d</sup>\n",
    "* a = number of recursive calls made by the algorithm = number of children each problem has = __rate at which subproblems proliferate as we proceed deeper into the tree__ = the factor by which there are more subproblems at the next level than the previous one --> ___RSP___\n",
    "    * Could be considered \"forces of evil\" because this is what is likely to make your function run more slowly (as the recursion generates more and more subproblems)\n",
    "* b<sup>d</sup> = __Rate of work shrinkage per subproblem__ = ___RWS___\n",
    "    * \"forces of good\" - what will allow your function to move more quickly, because the amount of work being completed is reduced\n",
    "#### Tug of war between forces of good and evil has 3 possible outcomes\n",
    "1. Forces match --> a = b<sup>d</sup> --> The  amount of work is the same at every recursion level \n",
    "2. Evil wins --> a > b<sup>d</sup> --> The amount of work increases with recursion level\n",
    "3. Good wins --> a < b<sup>d</sup> --> The amount of work decreases at every recursion level \n",
    "#### We know that the upper bound for level j is:\n",
    "<img src=\"resources/master_sum.PNG\">\n",
    "* Given this, we can intuitively derive the formulas for our 3 cases\n",
    "* Case 1: Where RSP = RWS (a = b<sup>d</sup>), we know that the work is constant per level, so we just need to account for the number of levels, j --> T(n) = O(n<sup>d</sup>logn))\n",
    "    * If these two epxressions are equal, notice in the formula above that the expression a/b<sup>d</sup> is now equal to 1 for all j, meaning that when we go to calculate the total work, 1<sup>j</sup> of course = 1, and then the sum is just 1 summed with itself log<sub>b</sub>n + 1 times. This then gets multiplied by the cn<sup>d</sup> term, which is independent of j. For  big-oh, we suppress the constant, _c_, the log base (because all logarithms differ by a constant factor), and the last + 1\n",
    "* Case 2: Where RSP < RWS (a < b<sup>d</sup>), we know that there is less work at each level, so the most work is being done at the root --> T(n) = O(n<sup>d</sup>)\n",
    "* Case 3: Where RSP > RWS (a > b<sup>d</sup>), we know that there is _more_ work being done at each level, so the most work is happening at the leaves --> O(# leaves) --> T(n) = O(n<sup>log<sub>b</sub>a</sup>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-relevance",
   "metadata": {},
   "source": [
    "## Proof Part 2 <a class=\"anchor\" id=\"p2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-china",
   "metadata": {},
   "source": [
    "#### Short detour\n",
    "* r = a/b<sup>d</sup>\n",
    "* Sum up the powers of r stopping at the _k_ th power:\n",
    "    * 1 + r + r<sup>2</sup> + r<sup>3</sup> + ...r<sup>k</sup> \n",
    "    * For this expression there is a formula that will represent the solution  \n",
    "    <br>\n",
    "<img src = \"resources/master_p2.PNG\">\n",
    "* For r < 1, r <= 1/1-r\n",
    "    * Consider r = 1/2 --> the sum of the powers (1/2, 1/4, 1/8...) is converging to 2 as k grows large --> 1 / (1/2) = 2\n",
    "    * ___Note that for r < 1, the sum here is a constant (that is, independent of k)___\n",
    "    * Another way to think about this is that for r < 1, the  first term (1) dominates, and regardless of k, the sume never exceeds the sum constant above\n",
    "* For r > 1, r <= r<sup>k</sup> * 1 + (1/(r - 1))\n",
    "    * Consider r = 2 --> the sume of the powers (2, 4, 6...)will never be greater than _twice_ the largest term\n",
    "    * ___The second part of this expression (1 + 1/r-1) is also independent of k___\n",
    "    * Another way to think about this is that for r < 1, the _largest_ term dominates (r<sup>k</sup>)\n",
    "* __To summarize, if r > 1, the largest power of that constant will dominate the sum, where as if r < 1, that sum is just a constant__\n",
    "#### Back to the Master Method\n",
    "* Case 2: a < b<sup>d</sup> (that is, r < 1 and work is decreasing with each level)\n",
    "    * While r does depend on a, b, and d, it is a constant in that it doesn't depend on n.\n",
    "    * Since r < 1, we know that the upper bound is going to be a constant independent of the number of terms (ie: n)\n",
    "    * Our general formula simplifies to c * n<sup>d</sup> * r\n",
    "    * Suppress constants for big-oh --> __O(n<sup>d</sup>)__\n",
    "    * Thus, this type of recursion tree is dominated by the root\n",
    "* Case 3: a > b<sup>d</sup> (that is, r > 1 and work is increasing with each level)\n",
    "    * Our general formula simplifies to c * n<sup>d</sup> * an additional constant factor, the dominating largest term (ie: the biggest value of j we will ever see)\n",
    "    * Big-oh --> O(n<sup>d</sup> * r<sup>log<sub>b></sub>n</sup> --> O(n<sup>d</sup> * (a/b<sup>d</sup>)<sup>log<sub>b></sub>n</sup>\n",
    "    * Simplifying the second term\n",
    "        * Consider only the denominator of the r ratio (1/b<sup>d</sup>)<sup>log<sub>b></sub>n</sup>\n",
    "        * b<sup>(-d)log<sub>b</sub>n</sup> = (b<sup>log<sub>b</sub>n</sup>)<sup>-d</sup>\n",
    "        * b and log<sub>b</sub> cancel--> n<sup>-d</sup>\n",
    "   * Substitute it back in:\n",
    "       * O(n<sup>d</sup> * a<sup>log<sub>b</sub>n</sup>/n<sup>d</sup>) --> ___O(a<sup>log<sub>b</sub>n</sup>)___\n",
    "   * log<sub>b</sub>n = the highest level j --> a<sup>j</sup> we already know is the number of subproblems per level j --> the last expression is really just the number of leaves\n",
    "    * __Why is this different than the given Case 3 formula of n<sup>log<sub>b</sub>a</sup>?__\n",
    "        * a<sup>log<sub>b</sub>n</sup> and n<sup>log<sub>b</sub>a</sup> are equivalent\n",
    "        * While a<sup>log<sub>b</sub>n</sup> is more intuitive, n<sup>log<sub>b</sub>a</sup> is easier to apply"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
