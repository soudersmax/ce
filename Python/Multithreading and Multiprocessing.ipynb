{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "driven-prevention",
   "metadata": {},
   "source": [
    "# Multithreading and Multiprocessing\n",
    "\n",
    "1. [Programs, Processes, and Threads](#intro)\n",
    "2. [When to Choose Threads vs. Multiple Processes](#choose)\n",
    "3. [Global Interpreter Lock](#gil)\n",
    "    * [Original Purpose](#purpose)\n",
    "    * [Impact on Multi-Threaded Python Programs](#impact)\n",
    "    * [Alternatives to Multi-Threading in Python](#alt)\n",
    "<br>\n",
    "<br>\n",
    "4. [Concurrency](#conc)\n",
    "    * [Concurrency and Parallelism](#cp)\n",
    "    * [When is Concurrency Useful?](#use)\n",
    "<br>\n",
    "<br>\n",
    "5. [Multiprocessing](#multi)\n",
    "    * [Advantages of Multiprocessing](#multi2)\n",
    "<br>\n",
    "<br>\n",
    "6. [Threading in Python](#thread)\n",
    "    * [More on Threads](#thread2)\n",
    "    * [Using a ThreadPoolExecutor](#thread3)\n",
    "    * [Race Conditions](#race)\n",
    "        * [Solving Race Conditions via Basic Synchronization using Lock](#thread4)\n",
    "        * [Deadlock](#dead)\n",
    "    * [Producer-Consumer Threading](#thread6)\n",
    "        * [The Lock Method](#pct1)\n",
    "        * [The Queue Method](#pct2)\n",
    "    * [Threading Objects](#thread7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-dealer",
   "metadata": {},
   "source": [
    "## Programs, Processes, and Threads  <a class=\"anchor\" id=\"intro\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-member",
   "metadata": {},
   "source": [
    "* Programs are loaded into the machine's non-valtile memory in binary form\n",
    "* Programs obviously need memory and various OS resources to run\n",
    "    * Registers - data holding places that are part of the CPU. May hold instructions, storage address, or some other process-dependent data\n",
    "    * Program Counter (or Instruction Pointer) - keeps track of where a computer is in its program sequence\n",
    "    * Stack - data structure that stores information about the active subroutines of a computer program and used as a scratch space for the process\n",
    "    * Heap - dunamically allocated memory for a process\n",
    "* __Process__ - a program that has been loaded into memory _along with all the resources it needs to operate_\n",
    "    * Each instance of a program is its own process which runs independently and isolated from other processes\n",
    "    * Cannot directly access shared data in other processes\n",
    "    * Switching processes requires some time (relatively speaking) for saving and loading registers, memory maps, and other resources\n",
    "    * This independence helps ensure that a problem with one process doesn't corrupt or otherwise mess with another process\n",
    "<br>\n",
    "<img src=\"resources/threads/proc_thread.PNG\">\n",
    "<br>\n",
    "* __Threads__ - Unit of execution _within_ a process.\n",
    "    * Can vary from 1 to many for a single process\n",
    "    * Each thread in a process shares the processes allocated memory and resources.\n",
    "    * ___Each thread has its own stack, but all threads in a process will share the heap___\n",
    "    * Sometimes called lightweight processes because of their dedicated stack. \n",
    "    * Share the same address space as the process and other threads, so communication is quick and easy\n",
    "    * A problem with one thread in a process will definitely affect other threads and the viability of the process as a whole\n",
    "* Single-threaded processes contain only 1 thread, meaning that the process and the thread are one and the same - there is only one thing happening\n",
    "* Multithreaded processes contain more than one thread and is accomplishing a number of things at (almost) the same time\n",
    "<br>\n",
    "<img src=\"resources/threads/single_multi.PNG\">\n",
    "<br>\n",
    "* Systems with multiple processors or CPU cores (common in modern processors) can execute multple processes or threads in parallel, but single processors can not\n",
    "    * In the case of single processors, they run processes via a process scheduling algorithm that divides the CPU's time and gives the illusion of parallel execution (called _concurrency_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-hacker",
   "metadata": {},
   "source": [
    "## When to Choose Threads vs. Multiple Processes <a class=\"anchor\" id=\"choose\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-speed",
   "metadata": {},
   "source": [
    "* Tradeoffs between memory and resources and potentially speed  \n",
    "* ex: Google Chrome, unlike most other browsers, runs each tab in its own process, rather than threads of a single process. \n",
    "    * Higher fixed cost in memory and resources but less memory bloat overall  \n",
    "    * Better memory usage when memory is low \n",
    "        * inactive window is treated as a lower priority by the OS and becomes eligible to be swapped to disk when memory is needed for other processes\n",
    "<br>\n",
    "<img src=\"resources/threads/adv_disadv.PNG\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-slovak",
   "metadata": {},
   "source": [
    "## Python's Global Interpreter Lock (GIL) <a class=\"anchor\" id=\"gil\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-feedback",
   "metadata": {},
   "source": [
    "* Mutex (lock) that allows only one thread to hold control of the python interpreter\n",
    "    * AKA - only one thread can be in a state of execution at any point in time\n",
    "    * MUTEX means Mutually Exclusive Flag\n",
    "* Performance bottleneck in CPU-bound and multi-threaded code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-alaska",
   "metadata": {},
   "source": [
    "### Original Purpose <a class=\"anchor\" id=\"purpose\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-execution",
   "metadata": {},
   "source": [
    "* Python usees reference counting for memory management\n",
    "    * Objects in python have a reference count variable that keeps track of the number of references that point to the object\n",
    "    * When that count hits 0, the memory occupied by the object is released\n",
    "* The reference count variable needed protection from [__race conditions__](#race) where two threads increase or decrease its value simultaneously\n",
    "    * Consequences: Leaked memory that's never released, incorrectly released memory, or worse\n",
    "    * Locks could be added to all data structures that are shared across threads to prevent them from being modified inconsistently\n",
    "        * Having multiple locks can lead to __Deadlock__ \n",
    "        * Also decreased performance by repeated acquisition and release of locks\n",
    "* GIL is a single lock on the interpreter itself which adds a rule that execution of any Python bytecode requires acquiring the interpreter lock\n",
    "    * Prevents deadlocks, since there's only one lock\n",
    "    * Doesn't add much performance overhead\n",
    "    * Effectively makes any program single threaded\n",
    "    * Pragmatic solution to what was a new problem at the time of development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-burning",
   "metadata": {},
   "source": [
    "### Impact on Multi-Threaded Python Programs <a class=\"anchor\" id=\"impact\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-album",
   "metadata": {},
   "source": [
    "* __CPU-bound Programs__ - push the CPU to its limit\n",
    "    * Includes programs that do mathematical computations like matrix multiplications, searching, image processing, etc.\n",
    "    * Effectively single-threaded because of GIL\n",
    "    * Will see an increase in execution time in comparison to an entirely single-threaded version of the same process\n",
    "    * Typically better dealt with by multiprocessing\n",
    "* __I/O-bound Programs__ - spend time waiting for input/output which can come fro ma user, file, database, netwoork, etc.\n",
    "    * May have to wait for significant amounts of time for input due to source doing its own processing\n",
    "    * Not highly impacted by GIL as the lock is shared between threads while they wait for the I/O\n",
    "    * Frequently arise when working with something much slower than your CPU (typically file system and network connections)\n",
    "    * Typically better dealt with using Threading\n",
    "* There are also programs  with threads of each type,so Python implemented a force release\n",
    "    * After a fixed interval of continuous use, a thread is forced to release the GIL and if no other thread acquires it , the same thread can continue usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-miami",
   "metadata": {},
   "source": [
    "### Alternatives to Multi-Threading in Python <a class=\"anchor\" id=\"alt\"></a>                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-exposure",
   "metadata": {},
   "source": [
    "* Multi-processing vs multi-threading\n",
    "    * Each python process gets its own python interpreter and memory space ,so the GIL isn't a problem\n",
    "    * Python has a `multiprocessing` module that does this easily\n",
    "* Alternative Python interpreters\n",
    "    * There are multple interpreter implementations written in a number of languages. If your program and its libraries is available for another implementation, you can try that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-harmony",
   "metadata": {},
   "source": [
    "## Concurrency <a class=\"anchor\" id=\"conc\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-instrument",
   "metadata": {},
   "source": [
    "### Concurrency and Parallelism <a class=\"anchor\" id=\"cp\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-canada",
   "metadata": {},
   "source": [
    "* __Concurrency__ - things happening simultaneously\n",
    "    * In Python, those things have many different names (thread, task, process) but at a high level refer to instructions running in order.\n",
    "* Different libraries that provide Python workarounds for allowing concurrency do so in different ways. Only in `multiprocessing` are events truly happening at the same time\n",
    "* Pre-emptive multitasking - OS can pre-empt your thread to make a switch to a different thread\n",
    "    * Used by `threading`\n",
    "    * Don't have to hard code anything into the program to make the switch\n",
    "    * Switch can happen at _any time_ even in the middle of trivial statements\n",
    "* Cooperative multitasking - tasks cooperate by 'announcing' when they are ready to be switched out\n",
    "    * Requires slight code change but you know switching will happen at specific times\n",
    "     * Used by `asyncio`\n",
    "* __Parallelism__ - Multiple processes running at the same time on different CPU cores\n",
    "    * `multiprocessing` creates new processes\n",
    "    * Because each process is different, they can run on a different core in true concurrency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-georgia",
   "metadata": {},
   "source": [
    "## Multiprocessing <a class=\"anchor\" id=\"multi\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-graphics",
   "metadata": {},
   "source": [
    "* Multiprocessing refers to the ability of a system to support more than one processor at the same time\n",
    "    * Applications are broken to smaller chunks of code that run independently.\n",
    "    * These chunks (threads) are allocated to multiple processors, improving prerformance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-hearing",
   "metadata": {},
   "source": [
    "### Advantages of Multiprocessing <a class=\"anchor\" id=\"multi2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-philip",
   "metadata": {},
   "source": [
    "1. Increased Throughput - By increasing the number of processors more work can be completed in the same time\n",
    "2. Cost Saving - Parallel system shares the memory, buses, peripherals, etc. Also, if a number of programs operate on the same data, it is cheaper to store that data on one single disk and shared by all rather than using many copies\n",
    "3. Increased reliability - If one processor fails then its failure may slightly slow down the speed of the system, but it will still work smoothly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-scoop",
   "metadata": {},
   "source": [
    "## Threading in Python <a class=\"anchor\" id=\"thread\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-indiana",
   "metadata": {},
   "source": [
    "* Python has a `threading` module which provides a lot of functionality in this realm\n",
    "    * For a Python Threading Tutorial, see [this notebook](#http://localhost:8888/notebooks/Python/threading_tutorial.ipynb#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-moldova",
   "metadata": {},
   "source": [
    "### More On Threads <a class=\"anchor\" id=\"thread2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-clothing",
   "metadata": {},
   "source": [
    "* __Daemon Threads__ - shuts down immediately when a program exits\n",
    "    * Can be considered a thread that runs in the background without worrying about having to manually shut it down\n",
    "    * Programs running `Threads` that are not `daemons` will wait for those threads to complete before it terminates, whereas `daemon` threads are killed upon exiting\n",
    "* When you want to specifically wait for a thread to stop, call `.join()`\n",
    "    * Works for both ensuring that a thread is executed before a program closes and for making one thread wait until another is finished\n",
    "* The order in which threads are run is determined by the OS and can be hard to predict\n",
    "    * Can be coordinated in the `threading` module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-official",
   "metadata": {},
   "source": [
    "### Using a ThreadPoolExecutor <a class=\"anchor\" id=\"thread3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-credit",
   "metadata": {},
   "source": [
    "* Context manager which starts up a given number of worker threads\n",
    "    * Uses `map` to step through an iterable, passing each one to a thread in the pool\n",
    "    * Automativally does the `.join()` at the end\n",
    "    * Can cause soem confusing errors - the exceptions are not always shown, so a process may just terminate with no output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-opinion",
   "metadata": {},
   "source": [
    "### Race Conditions <a class=\"anchor\" id=\"race\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-content",
   "metadata": {},
   "source": [
    "* Occur when two or more threads access a shared piece of data or resource\n",
    "* Difficult to debug because they're typically rare and produce confusing results\n",
    "* Oversimplified explanation\n",
    "    * When you tell ThreadPoolExecutor to run each thread, you tell it which function to run and what parameters to pass to it\n",
    "    * The reuslt of this is that each of the threads in a pool will call that function, which calls an instance method on the target object\n",
    "    * Each thread will have a reference to the same object and will also have a unique index \n",
    "        * If you're only running a single thread, then there is (obviously) no concern for race conditions, so things like `time.sleep()` don't have an effect\n",
    "    * When the thread starts running the function, it has its own version of all the data local to the function, which means that all variables that are scoped to a function are __thread safe__, but the shared target object is the problem\n",
    "    * Essentially, the threads have interleaving access to a single shared object, overwriting each others' results\n",
    "        * Similar race conditions can arise when one thread frees memory or closes a file handle before the other thread is finished accessing it "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-liver",
   "metadata": {},
   "source": [
    "#### Solving Race Conditions via Basic Synchronization using Lock <a class=\"anchor\" id=\"thread4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-assist",
   "metadata": {},
   "source": [
    "* `Lock` essentially does the same thing as the GIL\n",
    "    * Mutex object \n",
    "    * Only one thread at a time can have the `Lock` object, and any others must wait until it is released\n",
    "    * Also operates as a context manager, so with certain syntax it will be released automatically when exited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-basketball",
   "metadata": {},
   "source": [
    "#### Deadlock <a class=\"anchor\" id=\"dead\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-flashing",
   "metadata": {},
   "source": [
    "* Describes the phenomenon occuring when a thread must wait for another thread to release the lock\n",
    "* Typical causes:\n",
    "    1. An implementation bug where a lock is not released properly\n",
    "        * Reduce occurence by utilizing Lock as acontext manager\n",
    "    2. A design issue where a utility function needs to be called by functions that might or might not already have the lock\n",
    "        * Python has a second object designed to help with this (`RLock`) - essentially a fairness regulator for resource access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-murder",
   "metadata": {},
   "source": [
    "### Producer-Consumer Threading <a class=\"anchor\" id=\"thread 6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-youth",
   "metadata": {},
   "source": [
    "* Standard computer science problem used to look at threading or process synchronization issues\n",
    "* Producer - generates an output that is the consumer's input\n",
    "* Consumer - inputs the Producer's output\n",
    "* Between the two, there is a pipeline that transmits the relevant data\n",
    "    * The pipeline can be altered to use different synchronization methods, like locks or queues\n",
    "* For the lock method, there are two locks, `consumer_lock` and `producer_lock` which manager information transmission through the pipeline\n",
    "* ___EXAMPLE___:\n",
    "* Producer - Imagine a program that needs to read messages from a network and write them to a disk\n",
    "    * Does not request a message when it wants, but must be 'listening' and accepting  messages as they come\n",
    "    * Messages arrive in bursts, not regular pace\n",
    "* Consumer - Imagine a program that writes the messages to a database\n",
    "    * Access is slow, but fast enough to keep up to the average pace of messages\n",
    "    * _Not_ fast enough to keep up with a burst of messages\n",
    "* Between producer and consumer, there exists a pipeline \n",
    "    * This is what will change based on the synchronization objects "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-science",
   "metadata": {},
   "source": [
    "#### The Lock Method <a class=\"anchor\" id=\"pct1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-waste",
   "metadata": {},
   "source": [
    "* In this case, the Pipeline is just a class made to allow a single element between producer and consumer\n",
    "1. Initialize pipeline with Producer, Consumer, and Message members and acquire the consumer lock\n",
    "    * Essentially, the producer is allowed to put messages in the pipeline, but the consumer can't do any work until a message is present\n",
    "2. Once the consumer has acquired the `.consumer_lock` it copies the message and then releases the `producer_lock`, allowing the producer to insert the next message into the pipeline\n",
    "3. On the other side, the producer will notify the pipeline that it has a new message, acquire the `producer_lock`, send the message, then release the `consumer_lock` which allows the consumer to read the message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-dictionary",
   "metadata": {},
   "source": [
    "#### The Queue Method <a class=\"anchor\" id='pct2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-miami",
   "metadata": {},
   "source": [
    "* Allows more than one value in the pipeline at a time because it grows and shrinks as data backs up from the producer\n",
    "* Change Pipeline to a queue\n",
    "    * The queue has a maxsize to limit memory usage, and is inherently thread-safe\n",
    "1. `Event` allows one thread to signal an event while many other threads can be waiting for that event to happen\n",
    "    * Waiting threads check status every once in a while, don't need to stop what they're doing\n",
    "    * Producer and Consumer essentially loop until the event is triggered\n",
    "2. Producer creates a number of messages and is swapped out either by queue reaching maxsize or by the OS\n",
    "3. Consumer takes the messages from the pipeline\n",
    "4. Producer finishes adding all messages to the pipeline and exits\n",
    "5. Consumer continues its work until it has cleaned out the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-beginning",
   "metadata": {},
   "source": [
    "### Threading Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-letter",
   "metadata": {},
   "source": [
    "* __Semaphore__ - Atomic counter with special lock acquisition/release based behaviors\n",
    "    * AKA the OS can't swap the thread in the middle of incrementing/decrementing the counrer\n",
    "    * If a thread tries to acquire a lock while the counter is 0, it will block until a different thread releases and increments the counter to 1\n",
    "* __Timer__ - Schedules functions to be called after a certain amount of time has passed (but not necessarily at the exact desired time)\n",
    "* __Barrier__ - Keeps a fixed number of threads in sync\n",
    "    * Threads will be blocked until a specific number of them are waiting, then all are released at the same time\n",
    "    * Still scheduled to run one at a time\n",
    "    * Common use - Allows a pool of threads to initialize themselves, ensuring that none of the threads start running before all are finished with their initialization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
